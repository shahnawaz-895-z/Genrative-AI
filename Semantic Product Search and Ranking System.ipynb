{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First upgrade transformers\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "# Then modify the import line\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.optim import AdamW  # Get AdamW from PyTorch instead"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAj_pqdesOyk",
        "outputId": "811ab9d2-ddd0-43e9-b053-aef6dea1ea76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q transformers pandas numpy scikit-learn nltk torch faiss-cpu gradio matplotlib sentence-transformers\n",
        "import os\n",
        "if not os.path.exists('/content/esci-data'):\n",
        "    !git clone https://github.com/amazon-science/esci-data.git\n",
        "\n",
        "# [2] Imports\n",
        "#  [2] Corrected Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "import faiss\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ndcg_score\n",
        "from transformers import BertTokenizer, BertModel  # Modified import\n",
        "from torch.optim import AdamW  # Correct import location\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# [3] Data Preprocessing\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        tokens = text.split()\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
        "                 if token not in self.stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_data():\n",
        "    products = pd.read_parquet('/content/esci-data/shopping_queries_dataset/shopping_queries_dataset_products.parquet')\n",
        "    examples = pd.read_parquet('/content/esci-data/shopping_queries_dataset/shopping_queries_dataset_examples.parquet')\n",
        "\n",
        "    df = pd.merge(examples, products, on='product_id')\n",
        "    df['product_text'] = df['product_title'] + ' ' + df['product_description']\n",
        "\n",
        "    preprocessor = TextPreprocessor()\n",
        "    df['processed_query'] = df['query'].apply(preprocessor.clean_text)\n",
        "    df['processed_product'] = df['product_text'].apply(preprocessor.clean_text)\n",
        "    df['relevance'] = df['esci_label'].map({'E':3, 'S':2, 'C':1, 'I':0})\n",
        "\n",
        "    return df\n",
        "\n",
        "full_data = load_data()\n",
        "\n",
        "#  [4] Dataset Splitting\n",
        "train_data, temp_data = train_test_split(full_data, test_size=0.3, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "#  [5] BERT Model Setup\n",
        "class ProductSearchDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        query = self.tokenizer(\n",
        "            row['processed_query'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        product = self.tokenizer(\n",
        "            row['processed_product'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'query_input': {k: v.squeeze(0) for k, v in query.items()},\n",
        "            'product_input': {k: v.squeeze(0) for k, v in product.items()},\n",
        "            'label': torch.tensor(row['relevance'], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class BertRanker(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.classifier = torch.nn.Linear(768 * 2, 1)  # Concatenated features\n",
        "\n",
        "    def forward(self, query_input, product_input):\n",
        "        query_out = self.bert(**query_input).last_hidden_state[:,0,:]\n",
        "        product_out = self.bert(**product_input).last_hidden_state[:,0,:]\n",
        "        combined = torch.cat((query_out, product_out), dim=1)\n",
        "        return self.classifier(combined).squeeze()\n",
        "\n",
        "#  [6] Training Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertRanker().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "train_dataset = ProductSearchDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#  [7] Training Loop with Validation\n",
        "def train_model():\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(3):\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            query_input = {k: v.to(device) for k, v in batch['query_input'].items()}\n",
        "            product_input = {k: v.to(device) for k, v in batch['product_input'].items()}\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(query_input, product_input)\n",
        "            loss = torch.nn.MSELoss()(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for _, row in val_data.iterrows():\n",
        "                query_input = tokenizer(row['processed_query'], return_tensors='pt').to(device)\n",
        "                product_input = tokenizer(row['processed_product'], return_tensors='pt').to(device)\n",
        "                output = model(query_input, product_input)\n",
        "                val_loss += torch.nn.MSELoss()(output, torch.tensor([row['relevance']]).to(device)).item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_data)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(val_losses, label='Validation')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "train_model()\n",
        "\n",
        "#  [8] Evaluation Metrics\n",
        "def evaluate_model(test_df):\n",
        "    model.eval()\n",
        "    all_scores = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, row in test_df.iterrows():\n",
        "            query_input = tokenizer(row['processed_query'], return_tensors='pt').to(device)\n",
        "            product_input = tokenizer(row['processed_product'], return_tensors='pt').to(device)\n",
        "            score = model(query_input, product_input).cpu().item()\n",
        "            all_scores.append(score)\n",
        "            all_labels.append(row['relevance'])\n",
        "\n",
        "    # Calculate metrics\n",
        "    ndcg = ndcg_score([all_labels], [all_scores])\n",
        "    precision_at_10 = sum(np.array(sorted(zip(all_scores, all_labels), reverse=True)[:10])[:,1] >= 2) / 10\n",
        "\n",
        "    return {\n",
        "        'NDCG@10': ndcg,\n",
        "        'Precision@10': precision_at_10,\n",
        "        'Average Precision': sum(all_labels) / len(all_labels)\n",
        "    }\n",
        "\n",
        "print(\"Test Metrics:\", evaluate_model(test_data))\n",
        "\n",
        "#  [9] FAISS Indexing for Fast Retrieval\n",
        "def create_faiss_index(products_df):\n",
        "    model.eval()\n",
        "    index = faiss.IndexFlatIP(768)\n",
        "\n",
        "    product_embeddings = []\n",
        "    for _, row in products_df.iterrows():\n",
        "        inputs = tokenizer(row['processed_product'], return_tensors='pt').to(device)\n",
        "        with torch.no_grad():\n",
        "            emb = model.bert(**inputs).last_hidden_state[:,0,:].cpu().numpy()\n",
        "        product_embeddings.append(emb)\n",
        "\n",
        "    product_embeddings = np.concatenate(product_embeddings)\n",
        "    faiss.normalize_L2(product_embeddings)\n",
        "    index.add(product_embeddings)\n",
        "    return index\n",
        "\n",
        "faiss_index = create_faiss_index(full_data)\n",
        "\n",
        "#  [10] Gradio Interface\n",
        "def semantic_search(query):\n",
        "    # Preprocess query\n",
        "    preprocessor = TextPreprocessor()\n",
        "    processed_query = preprocessor.clean_text(query)\n",
        "\n",
        "    # Get query embedding\n",
        "    inputs = tokenizer(processed_query, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        query_emb = model.bert(**inputs).last_hidden_state[:,0,:].cpu().numpy()\n",
        "\n",
        "    # Search FAISS index\n",
        "    faiss.normalize_L2(query_emb)\n",
        "    distances, indices = faiss_index.search(query_emb, 10)\n",
        "\n",
        "    # Format results\n",
        "    results = []\n",
        "    for idx, score in zip(indices[0], distances[0]):\n",
        "        product = full_data.iloc[idx]\n",
        "        results.append({\n",
        "            'Title': product['product_title'],\n",
        "            'Description': product['product_description'][:200] + '...',\n",
        "            'Relevance Score': f\"{score:.3f}\"\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Launch interface\n",
        "iface = gr.Interface(\n",
        "    fn=semantic_search,\n",
        "    inputs=gr.Textbox(label=\"Enter your product search query\"),\n",
        "    outputs=gr.JSON(label=\"Top 10 Products\"),\n",
        "    title=\"Semantic Product Search Engine\",\n",
        "    examples=[\"Wireless Bluetooth Headphones\", \"Organic Cotton T-Shirts\"]\n",
        ")\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzaYCFqHpPVd",
        "outputId": "19286c84-17cc-4510-a3a5-70383e351efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-025c3f7e2611>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#  [4] Dataset Splitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-025c3f7e2611>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_query'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_product'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'product_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relevance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'esci_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'S'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-025c3f7e2611>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-zA-Z\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  [markdown]\n",
        "# ## 1. Environment Setup & Data Loading\n",
        "# !pip install -q transformers datasets sentence-transformers gradio pandas nltk torchmetrics scikit-learn umap-learn plotly\n",
        "# !apt install git-lfs\n",
        "\n",
        "# %%\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import os\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "import gradio as gr\n",
        "from umap import UMAP\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# %%\n",
        "# Clone dataset repository\n",
        "!git clone https://github.com/amazon-science/esci-data.git\n",
        "\n",
        "# Load datasets\n",
        "df = pd.read_parquet(\"/content/esci-data/shopping_queries_dataset/shopping_queries_dataset_examples.parquet\")\n",
        "products = pd.read_parquet(\"/content/esci-data/shopping_queries_dataset/shopping_queries_dataset_products.parquet\")\n",
        "\n",
        "# Merge datasets\n",
        "data = df.merge(products, on='product_id')\n",
        "\n",
        "#  [markdown]\n",
        "# ## 2. Text Preprocessing Pipeline\n",
        "\n",
        "# %%\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = self.pattern.sub('', text)\n",
        "        text = text.lower().strip()\n",
        "        tokens = text.split()\n",
        "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# %%\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Process product text\n",
        "data['product_text'] = data['product_title'] + ' ' + data['product_description']\n",
        "data['processed_product'] = data['product_text'].apply(preprocessor.preprocess)\n",
        "\n",
        "# Process queries\n",
        "data['processed_query'] = data['query'].apply(preprocessor.preprocess)\n",
        "\n",
        "#  [markdown]\n",
        "# ## 3. Dataset & DataLoader\n",
        "\n",
        "# %%\n",
        "class ProductRankingDataset(Dataset):\n",
        "    def __init__(self, queries, products, labels):\n",
        "        self.queries = queries\n",
        "        self.products = products\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'query': self.queries[idx],\n",
        "            'product': self.products[idx],\n",
        "            'label': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# %%\n",
        "# Split data\n",
        "train_df, temp_df = train_test_split(data, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProductRankingDataset(\n",
        "    train_df['processed_query'].values,\n",
        "    train_df['processed_product'].values,\n",
        "    train_df['esci_label'].map({'E': 1, 'S': 1, 'C': 0, 'I': 0}).values  # Convert labels to binary\n",
        ")\n",
        "\n",
        "#  [markdown]\n",
        "# ## 4. BERT-based Semantic Search Model\n",
        "\n",
        "# %%\n",
        "class SemanticSearchBERT(nn.Module):\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.projector = nn.Linear(768, 256)\n",
        "\n",
        "    def forward(self, texts):\n",
        "        inputs = self.tokenizer(texts, padding=True, truncation=True,\n",
        "                              max_length=128, return_tensors='pt').to(device)\n",
        "        outputs = self.bert(**inputs).last_hidden_state\n",
        "        return self.projector(outputs[:, 0, :])  # CLS token projection\n",
        "\n",
        "    def encode(self, texts, batch_size=32):\n",
        "        self.eval()\n",
        "        embeddings = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch = texts[i:i+batch_size]\n",
        "                embeddings.append(self(batch))\n",
        "        return torch.cat(embeddings)\n",
        "\n",
        "# %%\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SemanticSearchBERT().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
        "\n",
        "#  [markdown]\n",
        "# ## 5. Training Loop with Negative Sampling\n",
        "\n",
        "# %%\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Anchor: queries\n",
        "        anchors = model(batch['query'])\n",
        "\n",
        "        # Positive: relevant products\n",
        "        positives = model(batch['product'])\n",
        "\n",
        "        # Negative: random samples\n",
        "        neg_indices = torch.randint(0, len(dataloader.dataset), (len(anchors),))\n",
        "        negatives = model(dataloader.dataset.products[neg_indices])\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(anchors, positives, negatives)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# %%\n",
        "# Train model\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "for epoch in range(3):\n",
        "    loss = train_epoch(model, train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "#  [markdown]\n",
        "# ## 6. Evaluation Metrics\n",
        "\n",
        "# %%\n",
        "def evaluate(model, test_df, k=10):\n",
        "    model.eval()\n",
        "    queries = test_df['processed_query'].tolist()\n",
        "    products = test_df['processed_product'].tolist()\n",
        "\n",
        "    # Encode all queries and products\n",
        "    query_embs = model.encode(queries)\n",
        "    product_embs = model.encode(products)\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = F.cosine_similarity(query_embs.unsqueeze(1), product_embs.unsqueeze(0), dim=-1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    map_metric = torchmetrics.RetrievalMAP()\n",
        "    ndcg_metric = torchmetrics.RetrievalNormalizedDCG()\n",
        "\n",
        "    scores = []\n",
        "    for i in range(len(test_df)):\n",
        "        sims = similarities[i]\n",
        "        labels = torch.tensor([1 if j == i else 0 for j in range(len(test_df))])\n",
        "        indices = torch.arange(len(test_df))\n",
        "\n",
        "        map_metric.update(sims, labels, indices)\n",
        "        ndcg_metric.update(sims, labels, indices)\n",
        "\n",
        "    return {\n",
        "        'MAP': map_metric.compute().item(),\n",
        "        'NDCG@10': ndcg_metric.compute().item()\n",
        "    }\n",
        "\n",
        "# %%\n",
        "metrics = evaluate(model, test_df)\n",
        "print(f\"Test Metrics: {metrics}\")\n",
        "\n",
        "# [markdown]\n",
        "# ## 7. Visualization of Embeddings\n",
        "\n",
        "# %%\n",
        "def visualize_embeddings(texts, embeddings):\n",
        "    reducer = UMAP(n_components=2)\n",
        "    reduced = reducer.fit_transform(embeddings.cpu().numpy())\n",
        "\n",
        "    fig = px.scatter(\n",
        "        x=reduced[:, 0], y=reduced[:, 1],\n",
        "        color=[t[:20] for t in texts],  # Use first 20 chars as label\n",
        "        title=\"Product Embedding Space\",\n",
        "        labels={'color': 'Product Text'}\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "# %%\n",
        "sample_texts = test_df['processed_product'].sample(100).tolist()\n",
        "sample_embs = model.encode(sample_texts)\n",
        "visualize_embeddings(sample_texts, sample_embs)\n",
        "\n",
        "# [markdown]\n",
        "# ## 8. Gradio Interface with FAISS Index\n",
        "\n",
        "# %%\n",
        "!pip install -q faiss-cpu\n",
        "\n",
        "# %%\n",
        "import faiss\n",
        "\n",
        "class ProductSearchEngine:\n",
        "    def __init__(self, model, products):\n",
        "        self.model = model\n",
        "        self.products = products\n",
        "        self.index = faiss.IndexFlatIP(256)\n",
        "        self._build_index()\n",
        "\n",
        "    def _build_index(self):\n",
        "        product_embs = model.encode(self.products).cpu().numpy()\n",
        "        faiss.normalize_L2(product_embs)\n",
        "        self.index.add(product_embs)\n",
        "\n",
        "    def search(self, query, k=10):\n",
        "        query_emb = model.encode([query]).cpu().numpy()\n",
        "        faiss.normalize_L2(query_emb)\n",
        "        distances, indices = self.index.search(query_emb, k)\n",
        "        return [(self.products[i], d) for i, d in zip(indices[0], distances[0])]\n",
        "\n",
        "# %%\n",
        "search_engine = ProductSearchEngine(model, data['processed_product'].tolist())\n",
        "\n",
        "# %%\n",
        "def gradio_search(query):\n",
        "    results = search_engine.search(preprocessor.preprocess(query))\n",
        "    return {prod: float(score) for prod, score in results}\n",
        "\n",
        "# %%\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_search,\n",
        "    inputs=gr.Textbox(label=\"Enter product search query\"),\n",
        "    outputs=gr.Label(label=\"Top Matching Products\"),\n",
        "    examples=[\n",
        "        [\"Wireless noise cancelling headphones\"],\n",
        "        [\"Organic cotton t-shirt women's medium\"],\n",
        "        [\"4K ultra HD smart TV 55 inch\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "interface.launch(share=True)\n",
        "\n",
        "#  [markdown]\n",
        "# ## 9. Model Saving & Loading\n",
        "\n",
        "# %%\n",
        "# Save model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'tokenizer': model.tokenizer,\n",
        "}, 'semantic_search_model.pth')\n",
        "\n",
        "# Load model\n",
        "def load_model(path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model = SemanticSearchBERT()\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.tokenizer = checkpoint['tokenizer']\n",
        "    return model\n",
        "\n",
        "# %%\n",
        "# Example usage after loading\n",
        "loaded_model = load_model('semantic_search_model.pth')\n",
        "loaded_model.to(device)"
      ],
      "metadata": {
        "id": "iAcjCQtecRgy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}